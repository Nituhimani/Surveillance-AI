{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFPDGTPzCuSl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1yjIV-RDNXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyGeFrCtDNSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omo53LaEDNOA",
        "outputId": "099dcc4c-dcd9-48f7-dfe3-0724deaa65d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19582f8b"
      },
      "source": [
        "# Task\n",
        "Test the re-identification model \"/content/checkpoint_epoch25.pth\" by comparing two images and determining if they belong to the same person."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284a3e24"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained re-identification model from the `/content/checkpoint_epoch25.pth` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287eecd8"
      },
      "source": [
        "**Reasoning**:\n",
        "Import torch, define a basic model architecture, instantiate it, load the state dictionary from the checkpoint, and set the model to evaluation mode.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "5vvlXlthEsBQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0c679a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the model failed because the defined `SimpleReIDModel` architecture does not match the architecture saved in the checkpoint file. The error message indicates the keys in the state dictionary are different. To successfully load the model, I need to define a model architecture that matches the one used to create the checkpoint. Without knowing the exact architecture, I will try to inspect the keys in the loaded checkpoint to get hints about the actual architecture and try to define a more appropriate model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load('/content/checkpoint_epoch26.pth')\n",
        "\n",
        "# Print the keys in the state_dict to inspect the architecture\n",
        "print(checkpoint.keys())\n",
        "# print(checkpoint['net'].keys()) # Removed the line causing the error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P7gMVHTF__e",
        "outputId": "7c5c6803-cec6-4052-fb03-2d30ae71946f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['tied_convolution.0.weight', 'tied_convolution.0.bias', 'tied_convolution.3.weight', 'tied_convolution.3.bias', 'patch.0.weight', 'patch.0.bias', 'across_patch.0.weight', 'across_patch.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.2.weight', 'fc.2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Assuming train_csv_path and train_images_path are defined elsewhere\n",
        "# For demonstration, let's define placeholders if they are not in the notebook state\n",
        "try:\n",
        "    train_csv_path\n",
        "except NameError:\n",
        "    train_csv_path = '/content/train.csv' # Replace with actual path if needed\n",
        "try:\n",
        "    train_images_path\n",
        "except NameError:\n",
        "    train_images_path = '/content/train_images' # Replace with actual path if needed\n",
        "\n",
        "\n",
        "IMAGE_WIDTH = 60\n",
        "IMAGE_HEIGHT = 160\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Commenting out CustomDataset and related loading for now as the focus is on loading the model\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, df, root_path, transform=None):\n",
        "#         self.df = df.reset_index(drop=True)\n",
        "#         self.root = Path(root_path)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.loc[idx]\n",
        "#         img1 = Image.open(self.root / str(row[\"image1\"])).convert('RGB')\n",
        "#         img2 = Image.open(self.root / str(row[\"image2\"])).convert('RGB')\n",
        "\n",
        "#         if self.transform:\n",
        "#             img1 = self.transform(img1)\n",
        "#             img2 = self.transform(img2)\n",
        "\n",
        "#         label = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n",
        "#         return img1, img2, label\n",
        "\n",
        "# Load CSV\n",
        "# train_df = pd.read_csv(train_csv_path)\n",
        "# train_dataset = CustomDataset(train_df, train_images_path, transform=transform)\n",
        "\n",
        "\n",
        "class DNN_fixed(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tied_convolution = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(20, 25, kernel_size=5, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.patch = nn.Sequential(\n",
        "            nn.Conv2d(25, 25, kernel_size=5, stride=5),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.across_patch = nn.Sequential(\n",
        "            nn.Conv2d(25, 25, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, 2)\n",
        "        )\n",
        "        self.pad = nn.ZeroPad2d(2)\n",
        "\n",
        "    def get_f(self, f):\n",
        "        return F.interpolate(f, scale_factor=5, mode='nearest')\n",
        "\n",
        "    def get_g(self, y):\n",
        "        b, c, h, w = y.shape\n",
        "        y_p = self.pad(y)\n",
        "        patches = F.unfold(y_p, kernel_size=5, stride=1)\n",
        "        g = F.fold(patches, output_size=(h*5, w*5), kernel_size=5, stride=5)\n",
        "        return g\n",
        "\n",
        "    def cross_input_neighbourhood_difference(self, y1, y2):\n",
        "        return F.relu(self.get_f(y1) - self.get_g(y2))\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        y1 = self.tied_convolution(img1)\n",
        "        y2 = self.tied_convolution(img2)\n",
        "        y1_2 = self.cross_input_neighbourhood_difference(y1, y2)\n",
        "        y2_1 = self.cross_input_neighbourhood_difference(y2, y1)\n",
        "        y1 = self.patch(y1_2)\n",
        "        y1 = self.across_patch(y1)\n",
        "        y2 = self.patch(y2_1)\n",
        "        y2 = self.across_patch(y2)\n",
        "        y = torch.hstack((y1, y2))\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        logits = self.fc(y)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Instantiate the model using the provided DNN_fixed class\n",
        "model = DNN_fixed()\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load('/content/checkpoint_epoch26.pth')\n",
        "\n",
        "# Load the state dictionary from the checkpoint\n",
        "# Assuming the state_dict is directly in the checkpoint\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Oj1WKiF25V",
        "outputId": "c0cac24f-7253-40e8-cc6b-ab5b4fe320b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48fe7b84"
      },
      "source": [
        "## Test the model with two images\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the two test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9e7b298",
        "outputId": "3c5f664b-8418-4c68-e792-10afbc520f68"
      },
      "source": [
        "# Define paths to your test images\n",
        "# Replace with the actual paths to your images\n",
        "image1_path = '/content/5_039_2_09.png' # Example path\n",
        "image2_path = '/content/5_039_2_10.png' # Example path\n",
        "\n",
        "# Load the images\n",
        "try:\n",
        "    img1 = Image.open(image1_path).convert('RGB')\n",
        "    img2 = Image.open(image2_path).convert('RGB')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading image: {e}. Please check the image paths.\")\n",
        "    # You might want to stop execution here or handle the error differently\n",
        "    # For now, we'll just print the error and continue, which might lead to further errors\n",
        "    img1 = None\n",
        "    img2 = None\n",
        "\n",
        "\n",
        "# Apply the same transformations used during training\n",
        "if img1 is not None and img2 is not None:\n",
        "    img1_tensor = transform(img1)\n",
        "    img2_tensor = transform(img2)\n",
        "\n",
        "    # Add a batch dimension\n",
        "    img1_tensor = img1_tensor.unsqueeze(0)\n",
        "    img2_tensor = img2_tensor.unsqueeze(0)\n",
        "\n",
        "    print(\"Images loaded and preprocessed successfully!\")\n",
        "    print(f\"Image 1 tensor shape: {img1_tensor.shape}\")\n",
        "    print(f\"Image 2 tensor shape: {img2_tensor.shape}\")\n",
        "else:\n",
        "    print(\"Image loading failed. Cannot proceed with preprocessing.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images loaded and preprocessed successfully!\n",
            "Image 1 tensor shape: torch.Size([1, 3, 160, 60])\n",
            "Image 2 tensor shape: torch.Size([1, 3, 160, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is on the correct device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Move the image tensors to the same device as the model\n",
        "if img1_tensor is not None and img2_tensor is not None:\n",
        "    img1_tensor = img1_tensor.to(device)\n",
        "    img2_tensor = img2_tensor.to(device)\n",
        "\n",
        "    # Get the model output (logits)\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(img1_tensor, img2_tensor)\n",
        "\n",
        "    print(\"Model output obtained successfully!\")\n",
        "    print(f\"Model output (logits): {outputs}\")\n",
        "else:\n",
        "    print(\"Image tensors are not available. Cannot get model output.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVyip15MTvq",
        "outputId": "cbad9aaf-d3d3-4b50-c3ee-bc9e9435a098"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output obtained successfully!\n",
            "Model output (logits): tensor([[-4.6082,  2.3560]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpret the model output (logits) to get a prediction\n",
        "if 'outputs' in locals():\n",
        "    # Apply softmax to convert logits to probabilities\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    # Get the predicted class (the one with the highest probability)\n",
        "    # Assuming class 0 is 'different person' and class 1 is 'same person' based on common re-ID setups\n",
        "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Determine if the images are the same or different based on the predicted class\n",
        "    if predicted_class == 1:\n",
        "        prediction = \"The model predicts that the two images belong to the SAME person.\"\n",
        "    else:\n",
        "        prediction = \"The model predicts that the two images belong to DIFFERENT persons.\"\n",
        "\n",
        "    print(\"\\nInterpretation of Model Output:\")\n",
        "    print(f\"Probabilities: {probabilities.squeeze().tolist()}\") # Squeeze to remove batch dimension for cleaner printing\n",
        "    print(f\"Predicted class index: {predicted_class}\")\n",
        "    print(prediction)\n",
        "\n",
        "else:\n",
        "    print(\"Model output (logits) not available. Please run the previous cell first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulGZSstZOTH_",
        "outputId": "17c6d75f-d786-4d3f-8c88-74858173afa8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Interpretation of Model Output:\n",
            "Probabilities: [0.0009442267473787069, 0.999055802822113]\n",
            "Predicted class index: 1\n",
            "The model predicts that the two images belong to the SAME person.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# Define the same model architecture used for training\n",
        "class DNN_fixed(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tied_convolution = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(20, 25, kernel_size=5, stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.patch = nn.Sequential(\n",
        "            nn.Conv2d(25, 25, kernel_size=5, stride=5),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.across_patch = nn.Sequential(\n",
        "            nn.Conv2d(25, 25, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(500, 2)\n",
        "        )\n",
        "        self.pad = nn.ZeroPad2d(2)\n",
        "\n",
        "    def get_f(self, f):\n",
        "        return F.interpolate(f, scale_factor=5, mode='nearest')\n",
        "\n",
        "    def get_g(self, y):\n",
        "        b, c, h, w = y.shape\n",
        "        y_p = self.pad(y)\n",
        "        patches = F.unfold(y_p, kernel_size=5, stride=1)\n",
        "        g = F.fold(patches, output_size=(h*5, w*5), kernel_size=5, stride=5)\n",
        "        return g\n",
        "\n",
        "\n",
        "    def cross_input_neighbourhood_difference(self, y1, y2):\n",
        "        return F.relu(self.get_f(y1) - self.get_g(y2))\n",
        "\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        y1 = self.tied_convolution(img1)\n",
        "        y2 = self.tied_convolution(img2)\n",
        "        y1_2 = self.cross_input_neighbourhood_difference(y1, y2)\n",
        "        y2_1 = self.cross_input_neighbourhood_difference(y2, y1)\n",
        "        y1 = self.patch(y1_2)\n",
        "        y1 = self.across_patch(y1)\n",
        "        y2 = self.patch(y2_1)\n",
        "        y2 = self.across_patch(y2)\n",
        "        y = torch.hstack((y1, y2))\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        logits = self.fc(y)\n",
        "        return logits\n",
        "\n",
        "# Function to perform re-identification\n",
        "def reidentify_images(model_path, image1_path, image2_path):\n",
        "    # Define image transformations\n",
        "    IMAGE_WIDTH = 60\n",
        "    IMAGE_HEIGHT = 160\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Load the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = DNN_fixed()\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess the images\n",
        "    try:\n",
        "        img1 = Image.open(image1_path).convert('RGB')\n",
        "        img2 = Image.open(image2_path).convert('RGB')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading image: {e}. Please check the image paths.\", file=sys.stderr)\n",
        "        return None # Indicate failure\n",
        "\n",
        "    img1_tensor = transform(img1).unsqueeze(0).to(device)\n",
        "    img2_tensor = transform(img2).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get model output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img1_tensor, img2_tensor)\n",
        "\n",
        "    # Interpret output\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Determine if same or different\n",
        "    if predicted_class == 1:\n",
        "        prediction = \"SAME person\"\n",
        "    else:\n",
        "        prediction = \"DIFFERENT persons\"\n",
        "\n",
        "    return prediction, probabilities.squeeze().tolist()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    # Replace with the actual path to your checkpoint file and test images\n",
        "    model_checkpoint_path = '/content/checkpoint_epoch26.pth'\n",
        "    test_image1_path = '/content/5_039_2_09.png' # Replace with your image path\n",
        "    test_image2_path = '/content/5_039_2_10.png' # Replace with your image path\n",
        "\n",
        "    result, probabilities = reidentify_images(model_checkpoint_path, test_image1_path, test_image2_path)\n",
        "\n",
        "    if result:\n",
        "        print(f\"The model predicts that the two images belong to: {result}\")\n",
        "        print(f\"Probabilities (Different, Same): {probabilities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJg9Bk-NPAgJ",
        "outputId": "500d89bf-9ffd-403e-bf6a-70340b7ce6a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model predicts that the two images belong to: SAME person\n",
            "Probabilities (Different, Same): [0.0009442267473787069, 0.999055802822113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FiCI_R9NQdOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a7a548d"
      },
      "source": [
        "# Task\n",
        "Create a Gradio interface that takes two images as input and uses the `reidentify_images` function from the previously generated Python file to determine if they are the same person."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cce290"
      },
      "source": [
        "## Install gradio\n",
        "\n",
        "### Subtask:\n",
        "Install the `gradio` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbd90e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `gradio` library. I will use the `pip install` command in a code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83c49f82",
        "outputId": "cc93a165-62c8-4d0e-9dd6-d4510a9edeb6"
      },
      "source": [
        "!pip install gradio"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2de6b712"
      },
      "source": [
        "## Import necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Import the `gradio` library and the `reidentify_images` function from the Python file created in the previous step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c67711"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the gradio library and the reidentify_images function from the generated Python file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdb98cb"
      },
      "source": [
        "import gradio as gr\n",
        "from reidentify_images import reidentify_images"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "117d7916"
      },
      "source": [
        "## Define the gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Create a Gradio interface that takes two image inputs and uses the `reidentify_images` function as the core logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3978baf9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Gradio interface function and create the Gradio interface itself as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4321700"
      },
      "source": [
        "# Define the Gradio interface function\n",
        "def reid_interface(image1_path, image2_path):\n",
        "    # Call the reidentify_images function with the provided image paths\n",
        "    model_checkpoint_path = '/content/checkpoint_epoch26.pth' # Use the correct checkpoint path\n",
        "    result, probabilities = reidentify_images(model_checkpoint_path, image1_path, image2_path)\n",
        "\n",
        "    if result:\n",
        "        # Format the output for Gradio\n",
        "        output_string = f\"Prediction: {result}\\nProbabilities (Different, Same): [{probabilities[0]:.4f}, {probabilities[1]:.4f}]\"\n",
        "    else:\n",
        "        output_string = \"Error processing images. Please check the file paths.\"\n",
        "\n",
        "    return output_string\n",
        "\n",
        "# Create the Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=reid_interface,\n",
        "    inputs=[gr.Image(type=\"filepath\", label=\"Image 1\"), gr.Image(type=\"filepath\", label=\"Image 2\")],\n",
        "    outputs=gr.Text(label=\"Re-identification Result\"),\n",
        "    title=\"Person Re-identification Model\",\n",
        "    description=\"Upload two images to check if they belong to the same person.\"\n",
        ")\n",
        "\n",
        "# The interface can be launched using iface.launch() in a script,\n",
        "# but in a notebook environment, creating the interface object is sufficient for display.\n",
        "# iface.launch()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c460aab3"
      },
      "source": [
        "## Launch the interface\n",
        "\n",
        "### Subtask:\n",
        "Launch the Gradio application to make the interface available for testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f5e735"
      },
      "source": [
        "**Reasoning**:\n",
        "Launch the Gradio interface to make it accessible for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "267948fa",
        "outputId": "cf811ca2-3a85-487e-f3bc-2cc79699decb"
      },
      "source": [
        "# Launch the Gradio interface\n",
        "# In a Colab or similar environment, use share=True to get a public URL\n",
        "iface.launch(share=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5c874819c76a16a2af.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5c874819c76a16a2af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db2dbdc2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `gradio` library was successfully installed.\n",
        "*   The `gradio` library and the `reidentify_images` function were successfully imported.\n",
        "*   A Gradio interface was created to accept two image inputs and display the re-identification result.\n",
        "*   The Gradio interface was successfully launched, generating a public URL for access.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The created Gradio interface provides a user-friendly way to interact with the person re-identification model.\n",
        "*   Further testing should be conducted with various image pairs to evaluate the model's performance through the Gradio interface.\n"
      ]
    }
  ]
}